\documentclass[letterpaper]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath}
\usepackage{dsfont}
\usepackage{graphicx}
\usepackage[utf8]{inputenc}
\usepackage{xcolor}
% \usepackage[legalpaper, margin=2in]{geometry}
\parindent0pt
\parskip 1.5ex plus 1ex minus .5ex

\DeclareMathOperator{\Tr}{Tr}
\newcommand{\Ell}{\mathcal{L}}
\newcommand{\R}{\mathds{R}}
\newcommand{\C}{\mathds{C}}


\title{Open System Quantum Control}
\author{Stefanie G{\"u}nther}
% \affil{Lawrence Livermore National Laboratory, CA, USA}
% \date{}

\begin{document}
\maketitle

\section{Model equation}
We model open quantum systems with $Q$ oscillators with $n_k$ levels for the $k$-th oscillator, $k=1,\dots,Q$. We solve the Lindblad master equation
\begin{align}\label{mastereq}
  \dot \rho(t) = &-i(H(t)\rho(t) - \rho(t)H(t)) \notag \\
          &+ \sum_{l=1}^2 \sum_{k=1}^Q \gamma_{lk} \left( \Ell_{lk} \rho(t) \Ell_{lk}^{\dagger} - \frac 1 2 \left( \Ell_{lk}^{\dagger}\Ell_{lk} \rho(t) + \rho(t)\Ell_{lk}^{\dagger} \Ell_{lk}\right) \right)
\end{align}
for the density matrix $\rho(t)\in \C^{N\times N}$, $N := \prod_{k=1}^N n_k$. The Hamiltonian $H(t)$ consists of a constant \textit{drift} part, and a time-varying \textit{control} part. For the \textit{rotating frame}, these are computed from
\begin{align}
  H(t) &= H_d + H_c(t) \\
  \text{with} \quad H_d &:= \sum_{k=1}^Q \left(- \frac{\xi_k}{2} a_k^{\dagger}a_k^{\dagger}a_k a_k - \sum_{l\neq k} \xi_{lk} a_l^{\dagger}a_l a_k^{\dagger} a_k  \right) \\
                 H_c(t) &:= \sum_{k=1}^Q \left( p^k(\vec{\alpha}^k,t) (a_k + a_k^{\dagger}) + i q^k(\vec{\alpha}^k,t)(a_k - a_k^{\dagger})  \right)
\end{align}
Here, $a_k$ denotes the lowering operator:
\begin{align}
  \begin{array}{rl}
  a_1 &:= a^{(n_1)} \otimes I_{n_2} \otimes \dots \otimes I_{n_Q}\\
  a_2 &:= I_{n_1} \otimes a^{(n_2)} \otimes \dots \otimes I_{n_Q}\\
  \vdots \, & \\
  a_Q &:= I_{n_1} \otimes I_{n_2} \otimes \dots \otimes a^{(n_Q)}\\
  \end{array}
  \quad \text{with}\quad
 a^{(n_k)} = \begin{pmatrix}
   0 & 1 &          &         &    \\
     & 0 & \sqrt{2} &         &     \\
     &   & \ddots   & \ddots  &    \\
     &   &          &         & \sqrt{n_k-1}  \\
     &   &          &         & 0   
 \end{pmatrix} \in \R^{n_k \times n_k}
\end{align}
and $I_{n_k}$ denotes the identity matrix in $\R^{n_k \times n_k}$.

\subsection{Collapse operators}
The collapse operators $\Ell_{lk}$ in the Lindblad terms of the master equation \eqref{mastereq} can be of the following type:
\begin{itemize}
  \item ``T1'' -- Decay: $\Ell_{1k} = a_k$
  \item ``T2'' -- Dephasing: $\Ell_{2k} = a_k^{\dagger}a_k$
\end{itemize}
for each oscillator $k$. The constants $\gamma_{lk}$ are the inverse half-live for the corresponding collapse process $l$: $\gamma_{lk} = {\frac{1}{T_l}}$ (in nanoseconds, ns). Typical T1 decay time is between $10-100$ microseconds (us). T2 dephasing time is typically about half of T1 decay time. Decay then behaves like $\exp(-t/{T_1})$. Dephasing behaves like $\exp(-\frac{1}{2{T_2}})$.

\subsection{Control pulses}
The time-dependent control functions $p^k(\vec{\alpha}^k,t), q^k(\vec{\alpha}^k,t)$ are real-valued. We discretize them using B-splines with carrier waves:
\begin{align}
  p^k(\vec{\alpha}^k,t) &= \sum_{l=1}^L B_l(t) \sum_{f=1}^{N_f} \left(\alpha^{k (1)}_{l,f} \cos(\Omega_f^k t) - \alpha^{k (2)}_{l,f} \sin(\Omega_f^k t) \right) \\
  q^k(\vec{\alpha}^k,t) &= \sum_{l=1}^L B_l(t) \sum_{f=1}^{N_f} \left( \alpha^{k (1)}_{l,f} \sin(\Omega_f^k t) + \alpha^{k (2)}_{l,f} \cos(\Omega_f^k t) \right)
\end{align}
for $L$ B-Spline functions $B_l(t)$, and $N_f$ carrier wave frequencies $\Omega_f^k$. The amplitudes $\vec{\alpha}^k \in \R^{2LN_f}$ are the control parameters (\textit{design} variables) that we modify in order to realize a desired system behavior, giving a total of $2QLN_f$ design parameters:
\begin{align}
  z &:= \left( \vec{\alpha}^1, \dots, \vec{\alpha}^Q \right), \in \mathds{R}^{2QLN_f} \quad \text{where}\\
  \vec{\alpha}^k &= \left( \alpha_{1,1}^k,\dots, \alpha_{1,N_f}^k, \dots, \alpha_{L,1}^{k}, \dots, \alpha_{L,N_f}^k \right) \in \R^{2LN_f}, \quad \alpha_{l,f}^k = \left(\alpha_{l,f}^{k(1)}, \alpha_{l,f}^{k(2)} \right) \in R^2
\end{align}
The control functions are in the \textit{rotating frame}. To convert them back to the \textit{Lab frame} use
\begin{align}
  f^k(t) = 2 \sum_{l=1}^L B_l(t) \sum_{f=1}^{N_f} \beta_{l,f}^k \cos(w_r t + \Omega_f^k t + \theta_{l,f}) \quad \forall k=1,\dots Q
\end{align}
where $\beta_{l,f}^k$ and $\theta_{l,f}$ are computed from
\begin{align}
  \alpha_{l,f}^{k(1)} = \beta_{l,f}^k \cos(\theta_{l,f}) \quad \text{and} \quad \alpha_{l,f}^{k(2)} = \beta_{l,f}^k \sin(\theta_{l,f})
\end{align}
and $w_r$ is ??


\subsection{Measuring the expected energy level}
For each oscillator $k$, $k=1,\dots, Q$, we define the observable 
\begin{align}
  M^k := a_k^\dag a_k  = I_{n_1} \otimes \dots \otimes I_{n_{k-1}} \otimes  N_k \otimes I_{n_{k+1}} \otimes \dots I_{n_Q} 
\end{align}
       
       \quad
for the number operator 
\begin{align}
  N_k := a^{(n_k)^\dag} a^{(n_k)} = \begin{bmatrix} 
   0 &    &    & \\
     &  1 &    &\\
     &    &  2 &\\
     &    &    & \ddots 
  \end{bmatrix}
\end{align}
$M^k$ measures the probability of the energy levels of oscillator $k$. The expected energy level of oscillator $k$ is given by 
\begin{align}
  \langle M^k \rangle &= \sum_{m_k} m_k p(m_k)  = \mbox{Tr}(M^k\rho) = \mbox{Tr}(N_k \rho^k)  = \langle N_k\rangle
\end{align}
where $\rho^k$ is the reduced density matrix for oscillator $k$. Here, $p(m_k)$ is the probability of $\rho$ being the the $m_k$-th energy level for oscillator $k$, $p(m_k) = tr(P^k_m \rho)$ for a spectral decomposition $M^k = \sum_{m_k} m_kP^k_m$. 


\section{Optimization problem}

\subsection{Groundstate optimization}
We aim to find control pulses that drive any initial state towards the ground state $|0\dots 0\rangle = (1, 0, 0, \dots )^T$. The corresponding ground state density matrix is given by
\begin{align}
      \rho_{G} := |0\dots 0\rangle \langle 0 \dots 0 | = \begin{bmatrix} 1      & 0      &  \dots   \\ 
                                         0      & 0      &  \dots  \\ 
                                         \vdots & \vdots &  \dots 
                    \end{bmatrix}
\end{align}

We define two objective functions for groundstate optimization:
\begin{enumerate}
  \item Minimize the expected energy level:
    \begin{align}
      J(\rho(T)) = \sum_{k=1}^Q\langle M_k \rangle
    \end{align}
    where $M_k = a_k^\dag a_k$ is the number operator for oscillator $k$. 
  \item Minimize the frobenius norm between the final density matrix and the groundstate density matrix:
    \begin{align}
      J(\rho(T)) = \frac 12 \| \rho(T) - \rho_G \|^2_F 
    \end{align}
    where the Frobenius norm is defined as $\|A\|^2_F = \Tr(A^{\dagger}A)$.
\end{enumerate}
Both of the measures can be applied either to the full system $H = H_1\otimes \dots \otimes H_Q$ with density matrix $\rho$, or for a partial system $H_{k_1}\otimes \dots \otimes H_{K_s}$, with the reduced density matrix $\rho^{k_1, \dots, k_s}$.

\subsection{Target gate optimization}

Here, the goal is to find control pulses that realize a certain gate operation at final time $T$. Representing the gate by a unitary matrix $V\in \C^{N\times N}$, the goal is to match the final state $\rho(T)$ to the unitary transformation $V\rho(0)V^{\dagger}$ for any initial state $\rho(0)$:
\begin{align}
  J(\rho(T)) = \| \rho(T) - V\rho(0)V^{\dagger} \|^2_F 
\end{align} 


Target gates that we consider include
    \begin{align}
      V_{X} := \begin{bmatrix} 0 & 1 \\ 1 & 0  \end{bmatrix} \quad
      V_{Y} := \begin{bmatrix} 0 & -i \\ i & 0 \end{bmatrix} \quad
      V_{Z} := \begin{bmatrix} 1 & 0 \\ 0 & -1 \end{bmatrix} \quad 
      V_{Hadamard} := \frac{1}{\sqrt{2}} \begin{bmatrix} 1 & 1 \\ 1 & -1 \end{bmatrix} \\
      V_{CNOT} := \begin{bmatrix} 1  & 0 & 0 & 0 \\ 
                                   0  & 1 & 0 & 0 \\ 
                                   0  & 0 & 0 & 1 \\ 
                                   0  & 0 & 1 & 0 \\ 
                    \end{bmatrix}
    \end{align}



\subsection{Basis for initial conditions}
For both of the above optimization targets, the goal is to minimize the respective measure with respect to \textit{any} initial condition $\rho(0)$. We therefore define a basis for all initial density matrices and aim to minimize the above measure for all of the matrix basis elements. The basis matrices are defined as follows:

\begin{align}
B^{kj} := \frac 12 \left( E_{kk} + E_{jj}\right) +  \begin{cases} 
          0 & \text{if} \, k=j \\ 
        \frac 12 \left( E_{kj} + E_{jk}\right) & \text{if} \, k<j \\
        \frac i2 \left( E_{jk} - E_{kj}\right) & \text{if} \, k>j
      \end{cases} 
\end{align}
for all $k,j\in\{0,\dots, N-1\}$, where $E_{kj}$ denotes the matrix of all zeros except for the $(k,j)$-th element ($E_{kj} = e_ke_j^T$). These define $N^2$ hermitian matrices that are linearly independent over the real vector space of hermitian matrices. They further satisfy $\Tr(B^{kj}) = 1$ and are positive semi-definite, hence each basis matrix is a density matrix. 

We can rewrite any initial condition $\rho(0)$ as a linear combination 
\begin{align}
  \rho(0) = \sum_{i=0}^{N^2-1} = \alpha_i B^{k(i), j(i)}, \quad \text{where} \quad k(i) := i \,\mbox{mod}\, N, \quad \text{and} \quad j(i) := \left\lfloor \frac{i}{N} \right\rfloor
\end{align}
and, using a general solution operator $U(t)$ of the master equation \eqref{mastereq}, we can rewrite the final density matrix in terms of the basis elements:
\begin{align}
  \rho(T) = U(t) \rho_0 U(t)^\dag = \sum_{i=0}^{N^2-1} \alpha_i U(t)B^{k(i), j(i)}U(t)^\dag
\end{align}
We therefore define the optimization problem for all the basis elements, rather than a specific initial condition:
\begin{align}\label{optimproblem_basis}
  \min \frac{1}{N^2}\sum_{i=0}^{N^2-1} J(B^{i}(T)) 
  \quad \text{s.t.} \quad \text{all}\, B^{i}(t) \, \text{satisfy the master eq.\eqref{mastereq} with} \, B^i(0) = B^{k(i),j(i)}
\end{align}


\subsection{Fidelity} 
For closed systems, the $\rho(t)$ and the solution operator $U(t)$ are unitary. In that case, one can show that 
\begin{align}
  \frac{1}{2N^2}\|\rho(T) - V\rho(0)V\|_F^2 = 1 - \frac{1}{N^2}|\Tr(U(T)^{\dagger}V)|^2,
\end{align}
which is called the \textit{infidelity}, and the trace-term is the \textit{fidelity}. However, since we include the Lindblad terms, $\rho(T)$ is not unitary, and the above equation is not valid anymore. For defining a fidelity we consider $F(z) := 1 - J(z)$. Note, however, that if the optimized objective is in the order $O(\delta)$, then the error $\epsilon_i$ of a final state component $i$ from the gate is rather of the order $O(\sqrt{\delta})$, since $J$ represents the average squared error $J = \frac{1}{N^2}\sum_i \epsilon_i^2$ and so $\epsilon_i^2 \sim O(\delta)$ on average.

\section{Implementation}
  \subsection{Vectorization}

  The Lindblad master equation \eqref{mastereq} is in matrix form, describing the evolution of the densisy matrix $\rho = (\rho_1, \dots, \rho_N) \in \C^{N\times N}$. In order to solve this numerically, we vectorize the equation to receive an ODE for $q(t) := \text{vec}(\rho(t)) \in \C^{N^2}$. Using the relations
  \begin{align}
   \text{vec}(AB) &= (I_N\otimes A)\text{vec}(B) = (B^T\otimes I_N)\text{vec}(A) \\
   \text{vec}(ABC) &= (C^T\otimes A)\text{vec}(B)
  \end{align}
  for square matrices $A,B,C\in\C^{N\times N}$, we can derive the vectorized form of the Lindblad master equation:
  \begin{align}\label{mastereq_vectorized}
    &\dot q(t) = M(t) q(t) \quad  \text{where} \\
    &M(t) := -i(I_N\otimes H - H^T \otimes I_N) + \sum_{l,k=1}^{2,Q} \gamma_{lk} \left( \Ell_{lk}\otimes \Ell_{lk} - \frac 1 2 \left( I_N\otimes \Ell^T_{lk}\Ell_{lk} + \Ell^T_{lk}\Ell_{lk} \otimes I_N \right) \right)
  \end{align}

  \subsection{Real-valued system}
   We solve the vectorized master equation \eqref{mastereq_vectorized} in real-valued variables with $q(t) = u(t) + iv(t)$, evolving the real-valued states $u(t), v(t)\in \R^{N^2}$ with
   \begin{align}
     \dot q(t) &= M(t) q(t) \\
   \Leftrightarrow \quad \begin{bmatrix} \dot u(t) \\ \dot v(t) \end{bmatrix} &= 
   \begin{bmatrix} A(t) & -B(t) \\ B(t) & A(t) \end{bmatrix} \begin{pmatrix} u(t) \\ v(t) \end{pmatrix} \label{realvaluedODE}
   \end{align}
   where $M(t) = A(t) + i B(t)$, $A(t), B(t)\in \R^{N^2\times N^2}$. To assemble $A = Re(M)$ and $B = Im(M)$ consider
   \begin{align}
     -i(I_N \otimes H - H^T \otimes I_N) &= -I_N \otimes \left(iH_d + iH_c(t)\right) + \left(iH_d + iH_c(t)\right)^T \otimes I_N \\
     \text{and} \quad iH_d + iH_c(t) &= i H_d + i\left( \sum_k p^k(\vec{\alpha}^k,t)(a_k + a_k^{\dagger}) + iq^k(\vec{\alpha}^k,t)(a_k - a_k^{\dagger})\right) \\
                    &= - \sum_k q^k(\alpha^k,t)(a_k - a_k^{\dagger}) + i\left( H_d + \sum_k p^k(\alpha^k,t)(a_k+a_k^{\dagger}) \right) 
   \end{align}
   Hence $A$ and $B$ consist of a constant part $A_d, B_d$ and a time-varying part $A_c(t), B_c(t)$ with the following:
   \begin{align}
     A_d &=  \sum_{l,k=1}^{2,Q}\gamma_{lk} \left( \Ell_{lk}\otimes\Ell_{lk} - \frac 1 2 \left(I_N \otimes \Ell_{lk}^T\Ell_{lk} + \Ell_{lk}^T\Ell_{lk}\otimes I_N\right) \right)\\
     B_d &= -I_N \otimes H_d + H_d^T \otimes I_N \\
     A_c(t) &= \sum_k q^k(\vec{\alpha}^k,t) \underbrace{\left( I_N \otimes \left(a_k - a_k^{\dagger}\right) - \left(a_k - a_k^{\dagger}\right)^T\otimes I_N \right)}_{=:A_c^k} \\
     B_c(t) &= \sum_k p^k(\vec{\alpha}^k,t) \underbrace{\left( - I_N \otimes \left(a_k + a_k^{\dagger}\right) + \left(a_k + a_k^{\dagger}\right)^T\otimes I_N \right)}_{=:B_c^k} 
   \end{align}
   In the code, we initialize and store the constant matricees $A_d,B_d,A_c^k,  B_c^k$, and use them as building blocks to evaluate 
   \begin{align}
     A(t) &= Re(M(t)) = A_d + \sum_kq^k(\alpha^k, t)A_c^k \\
     B(t) &= Im(M(t)) = B_d + \sum_k p^k(\alpha^k, t)B_c^k
   \end{align}
   at time $t$, and finally build $M(t)$ from \eqref{realvaluedODE}.

  \subsection{Timestepper}
    The forward time-evolution can use any of Petsc's time-stepping integration schemes. However the adjoint time-evolution does can not. For this we rely on a hand-written time-stepper. Hand-written time-steppers include Backward-Euler, and (preferred) the \textit{implicit midpoint rule (IMR)}, based on implicit Runge-Kutta scheme. We prefer IMR because it is a simplectic integrator. Because IMR is self-adjoint, the same IMR scheme is used to solve the adjoint equation. 

    In order to choose a time-step size $\Delta t$, we do an eigenvalue analysis of the constant drift Hamiltonian $H_d =  -2\pi \sum_{k=1}^Q \frac{x_k}{2} a_k^{\dagger}a_k^{\dagger}a_ka_k + \sum_{l\neq k} x_kl a_l^{\dagger}a_l a_k^{\dagger}a_k$:
       \begin{align*}  
         \dot u = -i H_d u \qquad \text{with} \quad H_d^{\dagger}  = H_d
       \end{align*} 
       There exists a transformation $Y$ s.t. 
       \begin{align*}
         Y^{\dagger}H_d Y + \Lambda \qquad  \text{where} \quad Y^{\dagger} = Y
       \end{align*}
       where $\Lambda$ is a diagonal matrix containing the eigenvalues of $H_d$. Transform $\tilde u = Y^{\dagger} u$, then the ODE transforms to 
       \begin{align*}
         \dot \tilde u = -i \Lambda \tilde u \quad \Rightarrow \dot \tilde u_i = -i\lambda_i \tilde u_i \quad \Rightarrow \tilde u_i = a \exp(-i\lambda_i t)
       \end{align*}
       Therefore, the period for each mode is $\tau_i = \frac{2\pi}{|\lambda_i|}$, hence the shortest period is $\tau_{min} = \frac{2\pi}{\max_i\{|\lambda_i|}\}$. If we want $p$ discrete time points per period, then $p\Delta t = \tau_{min}$, hence 
       \begin{align*}
         \Delta t = \frac{\tau_{min}}{p} = \frac{2\pi}{p\max_i\{|\lambda_i|\}}
       \end{align*}
       Usually, for a first order scheme we would use something like $p=20$, second order maybe $p=10$. Ander's used $p=80$ for the test case. 

       If we want to include the time-varying Hamiltonian part $H = H_d + H_c(t)$ in the analysis, then we could use the constraints on the control parameter amplitudes to remove the time-dependency using their larges value instead and so the same analysis again. However this doesn't ensure that we resolve the time-scale of the control functions. 

  \subsection{Optimization}
    We use the \textit{HiOp} optimization package for solving the vectorized version of the optimal control problem \eqref{optimproblem_basis}. HiOp applies an interior point L-BFGS method. Each objective function evaluation requires to solve the initial value problem  
    \begin{align*}
      \dot x_i(t) &= M(t) x_i(t) \quad \forall \, t\in (0,T) \\
      x_i(0) &= \mbox{vec}(B^{k(i),j(i)})
    \end{align*}
    for the vectorized version of all basis elements, followed by the evaluation of the objective function 
    \begin{align}
      J(z) = \frac{1}{N^2} \sum_{i=0}^{N^2-1} J(x_i(T)) + \frac{\gamma}{2} \| z\|^2_2
    \end{align}
    where a Tikhonov regularization term has been added with parameter $\gamma>0$.

    In order to guaranty that the optimizer yields bounded control functions with $|p_k(t)| \leq c^k_{max}$, $|q_k(t)| \leq c^k_{max}$ for all oscillatorx $k=1,\dots, Q$, we impose box constraints on the control parameters:
        \begin{align}
          | \alpha_{l,f}^{k(1)}| \leq \frac{c^k_{max}}{N_f} \quad \text{and} \quad | \alpha_{l,f}^{k(2)} | \leq \frac{c^k_{max}}{N_f}
        \end{align}
        where $N_f$ is the number of carrier wave frequencies.


  \subsection{Parallelization}
    Four levels of parallelization are planned: 
      \begin{enumerate}
        \item Parallelization over the $N^2$ initial conditions
        \item Time-parallelization (XBraid)
        \item Spatial parallelism for parallel linear algebra (Petsc)
        \item Parallel optimization (HiOp)
      \end{enumerate}
      The global communicator (MPI\_COMM\_WORLD) is split into four sub-communicator, one for each of the above. Currently only the first two levels are implemented, while Petsc and HiOp run on communicators with size 1. The total number of mpi processes ($np_{total}$) is divided into cores for braid and for the initial conditions like so
         \begin{align*}
           np_{braid} * np_{init} = np_{total}.
         \end{align*}
      The user specifies the size of the communicator for distributing the initial conditions ($np_{init}$) in the config file, with the following requirements:
      \begin{itemize}
        \item $np_{init} \leq n_{init}$, where $n_{init}$ is the total number of initial conditions that are considered (can be $1$, $N$, or $N^2$). This requirement is handled by the code, limiting $np_{init}$ if necessary.
        \item $\frac{n_{init}}{np_{init}} \in \mathds{N}$, so that each processor group owns the same number of initial conditions.
        \item $\frac{np_{total}}{np_{init}} \in \mathds{N}$, so that each processor group has the same number of cores for braid, i.e. $np_{braid} = \frac{np_{total}}{np_{init}}$.
      \end{itemize}
  
\end{document}
