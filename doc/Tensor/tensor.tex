\documentclass[letterpaper]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath}
\usepackage{dsfont}
\usepackage{graphicx}
\usepackage[utf8]{inputenc}
\usepackage{xcolor}
% \usepackage[legalpaper, margin=2in]{geometry}
\parindent0pt
\parskip 1.5ex plus 1ex minus .5ex

\DeclareMathOperator{\Tr}{Tr}
\newcommand{\Ell}{\mathcal{L}}
\newcommand{\R}{\mathds{R}}
\newcommand{\C}{\mathds{C}}
\newcommand{\bfi}{\boldsymbol{i}}
\newcommand{\p}{\prime}
\newcommand{\Epk}{E_{+k}}
\newcommand{\Emk}{E_{-k}}
\newcommand{\pluseq}{\mathrel{+}=}


\title{Tensor contractions for the Lindblad master equation}
\author{Stefanie G{\"u}nther}
% \date{\today}

\begin{document}
\maketitle

\section{Tensor contractions}
Consider a matrix-matrix multiplication as rank-2 tensor contractions. For a general $A\in \R^{N\times N}$ and $\rho\in \R^{N\times N}$ the product $A\rho$ corresponds to a contraction of the rows of $A$ with the columns of $\rho$:
\begin{align}
    A \rho = y \quad \Leftrightarrow \quad y_{j_1,i_2} = A_{j_1}^{i_1} \rho_{i_1,i_2} = \sum_{i_1} A_{j_1}^{i_1} \rho_{i_1,i_2} \quad \forall j_1,i_2 = 0,\dots, N
\end{align}
where subscript and superscript indices correspond to rows and columns, respectively, $A_m^l = A_{m,l} = A(m,l)$. 

Note than when applying $A$ to $\rho$ from the right, one gets 
\begin{align}
    \rho A = y \quad \Leftrightarrow \quad y_{j_1,i_2} = \rho_{j_1}^{i_1} A_{i_1,i_2} = \sum_{i_1} \rho_{j_1}^{i_1} A_{i_1,i_2} \quad \forall j_1,i_2 = 0,\dots, n \\
   = \sum_{i_1} A_{i_1}^{i_2} \rho_{j_1,i_1} = \sum_{i_1} (A^T)_{i_2}^{i_1} \rho^T_{i_1,j_1} = A_{i_1}^{i_2} \rho_{j_1,i_1}
\end{align}
contracting the rows of $A$ with the rows of $\rho$. 

Now consider a kronecker product of operators $A\otimes B$ with dimensions $A\in\R^{n_a\times n_a}, B\in\R^{n_b\times n_b}$ multiplied to a matrix $\rho \in \R^{n_an_b\times n_bn_b}$ of matching dimensions. We can think of $\rho$ as a rank-4 tensor $\rho\in\R^{n_a\times n_b \times n_a \times n_b}$. The application of $A\otimes B$ to the tensor $\rho$ then corresponds to the tensor contraction 
\begin{align}
    z := A\otimes B \rho = A_{j_2}^{i_2}\otimes B_{j_1}^{i_1} \rho_{i_1, i_2, i_1^{\prime}, \i_2^{\prime}}
\end{align}
contracting first $B$ with $\rho$ on the first dimension $i_1$, then $A$ on the second dimension $i_2$:
\begin{align}
    y_{j_1, i_2, i_1^{\prime}, i_2^{\prime}} := B_{j_1}^{i_1} \rho_{i_1, i_2, i_1^{\prime}, i_2^{\prime}} = \sum_{i_1} B_{j_1}^{i_1} \rho_{i_1, i_2, i_1^{\prime}, i_2^{\prime}} \quad \forall j_1,i_2,i_1^\prime, i_2^\prime\\
    z_{j_1, j_2, i_1^{\prime}, i_2^{\prime}} := A_{j_2}^{i_2} y_{j_1, i_2, i_1^{\prime}, i_2^{\prime}} = \sum_{i_2} A_{j_2}^{i_2} y_{j_1, i_2, i_1^{\prime}, i_2^{\prime}} \quad \forall j_1,j_2,i_1^\prime, i_2^\prime
\end{align}
Similarly, when applying $A\otimes B$ from the right, the contraction acts on $i_2^\prime, i_1^\prime$, as above. 

More generally, for a tensor $\rho \in \R^{n_1\times \dots \times n_N}$ we can define the contraction of an operator $A$ on a certain dimension $i_l$ of $\rho_{i_1,\dots i_N}$:
\begin{align}
    y_{i_1,\dots,j_l,\dots,i_N} = A_{j_l}^{i_l} \rho_{i_1,\dots,i_l,\dots,i_N} = \sum_{i_l} A_{j_l}^{i_l} \rho_{i_1,\dots,i_l,\dots i_N} \quad \forall i_1,\dots, j_l,\dots,i_N
\end{align}



\section{Tensor contraction for Lindblad master equation}
Consider a quantum system with $Q$ oscillators in $H_{n_1} \otimes \dots H_{n_Q}$, with $n_k$ levels for the $k$-the oscillator and density matrix $\rho \in \C^{N\times N}$ with $N=\prod_{k=1}^{Q} n_k$. Consider the density matrix being a tensor of rank $2Q$: 
\begin{align}
    \rho_{i_1,\dots, i_Q, i_1^{\prime}, \dots, i_Q^{\prime}} =: \rho_{\bfi, \bfi^\prime}\in\C^{n_1\times \dots \times n_Q \times n_1 \times \dots \times n_Q}
\end{align}
using multi-indeces $\bfi := i_{1},\dots,i_{Q}$, and $\bfi^\prime = i_0^\prime, \dots, i_Q^\prime$. For ease of notation, we use the follwoing index shifting operators 
\begin{align}
  E_{+k} \bfi := i_0,\dots, i_{k}+1, \dots, i_Q \quad E_{+k} \bfi^\prime := i_0^\prime, \dots, i_k^\prime + 1, \dots, i_Q\\
  E_{-k} \bfi := i_0,\dots, i_{k}-1, \dots, i_Q \quad E_{-k} \bfi^\prime := i_0^\prime, \dots, i_k^\prime - 1, \dots, i_Q\\
\end{align}
to shift $i_k$ by one. 

All operations that are applied to $\rho$ in the Lindblad equation are of the form $I_{n_1}\otimes \dots \otimes M^{n_k} \otimes \dots \otimes I_{n_Q}$, or $I_{n_1}\otimes \dots \otimes M^{n_l} \otimes I_{n_{l+1}\dots n_{k-1}} \otimes M^{n_k} \otimes \dots \otimes I_{n_Q}$, where $I_{n_k}$ is the identity in $\R^{n_k}$. Applying those operators to the tensor $\rho$ therefore effectively acts in at most two of the dimensions, while others are contracted only with identities. Example for a 3-partite systems $H^{n_1} \otimes H^{n_2} \otimes H^{n_3}$. Consider the operator $I_{n_1} \otimes M_{n_2} \otimes I_{n_3}$ with corresponding dimension. It's action on $\rho \in \C^{n1\times n2\times n3 \times n1\times n2 \times n3}$ is
\begin{align}
    I^{i_1}_{j_1}\otimes M^{i_2}_{j_2} \otimes I^{i_3}_{j_3} \rho_{i_1i_2i_3i_1^{\prime}i_2^\prime i_3^\prime} = M^{i_2}_{j_2} \rho_{j_1i_2i_3j_1^{\prime}i_2^\prime i_3^\prime} = \sum_{i_2} M(j_2, i_2) \rho(j_1,i_2,j_3,i_1^{\prime},i_2^\prime, i_3^\prime)  =: y_{j_1,j_2,j_3,i_1^{\prime},i_2^\prime,i_3^\prime}
\end{align}
and when applying from the right
\begin{align}
    \rho_{i_1i_2i_3i_1^{\prime}i_2^\prime i_3^\prime}I^{i_1^{\prime}}_{j_1}\otimes M^{i_2^\prime}_{j_2} \otimes I^{i_3^\prime}_{j_3}  = M^{j_2^\prime}_{i_2^\prime} \rho_{i_1i_2i_3j_1^{\prime}i_2^\prime j_3^\prime} = \sum_{i_2^\prime} M(i_2^{\prime}, j_2^\prime) \rho(i_1,i_2,i_3,j_1^{\prime}i_2^\prime, j_3^\prime)  =: y_{i_1,i_2,i_3,j_1^{\prime}j_2^\prime, j_3^\prime}
\end{align}
Note that when applying from the right, the access of elements in $M$ is inverted, summing over columns in $M$.

\begin{enumerate}
    \item \textbf{Constant drift Hamiltonian} $H_d = -\sum_k \frac{\xi}{2} \left(N_k^2 - N_k\right) - \sum_{l<k} \xi_{lk} N_lN_k$, with
      \begin{align}
      N_k = I_{n_1}\otimes \dots \otimes N^{(n_k)} \otimes \dots \otimes I_{n_Q} \quad \text{with} \quad N^{(n_k)} := \begin{pmatrix} 0 & & \\ & 1& \\ & & \ddots \end{pmatrix}
      \end{align}
      Application to $\rho$ is hence a sum of contractions of $N^{(n_k)}$ to the corresponding $k$-th dimension of the tensor $\rho_{\bfi, \bfi^\p}$ , such as
      \begin{align}
        N_k \rho = \left(N^{(n_k)}\right)_{j_k}^{i_k} \rho_{\bfi, \bfi^\p} = \sum_{i_k} \underbrace{N^{(n_k)}(j_k, i_k)}_{ = j_k\delta_{j_ki_k} } \rho(\bfi, \bfi^\p) = i_k \rho_{\bfi, \bfi^\p}
      \end{align}
      And similarly
      \begin{align}
        \left(N_k^2-N_k\right)_{j_k}^{i_k} \rho_{\bfi, \bfi^\p} = \left(i_k^2 - i_k\right) \rho_{\bfi,\bfi^\prime}
      \end{align}

      The mixed term $N_lN_k$ expands to $N_k = I_{n_1}\otimes \dots \otimes N^{(n_l)} \otimes I_{n_{l+1} \dots n_{k-1}}\otimes N^{(n_k)} \otimes \dots \otimes I_{n_Q}$ and their tensor contraction results in products of the form 
      \begin{align}
        (N_l)^{i_l}_{j_l} \otimes (N_k)_{j_k}^{i_k} \rho_{\bfi, \bfi^\p} = i_li_k\rho(\bfi, \bfi^\prime)
      \end{align}

      Applying $H_d$ to the tensor $\rho_{\bfi, \bfi^p}$ from the left therefore gives  
      \begin{align}
        H_d \rho_{\bfi, \bfi^\p} = \underbrace{\left(-\sum_k \frac{\xi_k}{2}(i_k^2 - i_k) - \sum_{l<k} \xi_{lk} (i_l i_k) \right)}_{=:h_d(\bfi)} \rho_{\bfi, \bfi^\p}
      \end{align}
      Applying $H_d$ to $\rho$ from the right contracts on dimensions $\bfi^\prime$:
      \begin{align}
        \rho_{\bfi, \bfi^\p} H_d  = h_d(\bfi^\p) \rho_{\bfi,\bfi^\p}
        \end{align}

    \item \textbf{Time-dependent control Hamiltonian} $H_c(t) = \sum_k p^k(t) (a_k + a_k^\dag) + iq^k(a_k - a_k^\dag)$, with  
      \begin{align}
      a_k = I_{n_1}\otimes \dots \otimes a^{(n_k)} \otimes \dots \otimes I_{n_Q} \quad \text{with} \quad a^{(n_k)} := \begin{pmatrix} 0 & 1 & \\ & 0 & \sqrt{2} \\ & & & \ddots \end{pmatrix}
      \end{align}
      Each term contracts to the $k-$th dimension:
      \begin{align}
        (a_k\pm a_k^\dag) \rho_{\bfi,\bfi^\p} = (a^{(n_k}) \pm a^{(n_k)})^{i_k}_{j_k} \rho_{\bfi,\bfi^\prime} &= \sum_{i_k} \underbrace{(a^{(n_k)} \pm a^{(n_k)})_{j_k,i_k}}_{= \sqrt{j_k+1} \delta_{i_k,j_k+1} \pm \sqrt{j_k} \delta_{i_k,j_k-1}} \rho_{\bfi, \bfi^{\prime}} \\
        &= \sqrt{i_k+1} \rho_{E_{+k}\bfi, \bfi^\prime} \pm \sqrt{i_k} \rho_{E_{-k}\bfi, \bfi^\prime}.
      \end{align} 

      Applying $H_c(t)$ from the left then gives 
      \begin{align}
        H_c(t) \rho_{\bfi, \bfi^\prime} = \sum_k p^k(t) \left(\sqrt{i_k+1} \rho_{E_{+k}\bfi,\bfi^\prime} + \sqrt{i_k} \rho_{E_{-k} \bfi,\bfi^\prime} \right) + iq^k(t) \left(\sqrt{i_k+1} \rho_{E_{+k}\bfi,\bfi^\prime} - \sqrt{i_k} \rho_{E_{-k}\bfi,\bfi^\prime} \right)
      \end{align}
      Applying $H_c(t)$ from the right to $\rho$ contracts on the dimensions $\bfi^\prime$, and the matrix $(a^{n_k } \pm a^{n_k})$ is accessed at $(a^{n_k } \pm a^{n_k})_{i_k^\prime,j_k^\prime} = \pm \sqrt{j_k^\prime+1} \delta_{i_k^\prime, j_k^\prime +1} + \sqrt{j_k^\prime} \delta_{i_k^\prime j_k^\prime -1}$, giving
      \begin{align}
        \rho_{\bfi, \bfi^\prime} H_c(t) = \sum_k p^k(t) \left(\sqrt{i_k^\prime+1} \rho_{\bfi,E_{+k}\bfi^\prime} + \sqrt{i_k^\prime} \rho_{\bfi,E_{-k}\bfi^\prime} \right) + iq^k(t) \left( - \sqrt{i_k^\prime+1} \rho_{\bfi,E_{+k}\bfi^\prime} + \sqrt{i_k^\prime} \rho_{\bfi,E_{-k}\bfi^\prime} \right)
      \end{align}

    \item \textbf{Lindblad terms} $\sum_k \sum_{l\in{1,2}} \gamma_{lk} \Ell_{lk} \rho \Ell_{lk}^\dag - \frac{1}{2} \left( \Ell_{lk}^\dag \Ell_{lk} \rho + \rho \Ell_{lk}^\dag \Ell_{lk}\right)$.
    
    The dephasing collapse operator $\Ell_{2k} = a_k^\dag a_k$ is diagonal. Contraction with $\rho_{\bfi,\bfi^\prime}$ yields
    \begin{align}
        \sum_k \left( \gamma_{2k} i_ki_k^\prime - \frac 12 \left( i_k^2 + (i_k^\prime)^2\right) \right) \rho_{\bfi,\bfi^\prime}
    \end{align}

    The decay collapse operator $\Ell_{1k} = a_k$ contains off-diagonals, and yields
    \begin{align}
        \sum_k \gamma_{1k} \sqrt{(i_k+1)(i_k^\prime+1)} \rho_{E_{+k}\bfi, E_{+k}\bfi^\prime} - \frac 12 \left( i_k+ i_k^\prime \right)\rho_{\bfi, \bfi^\prime} 
    \end{align}
      
\end{enumerate}

\section{Implementation}

  To apply the above operators efficiently, collect those terms that access the same index set, such as $(\bfi,\bfi^\prime)$ vs. $(E_{+k}\bfi, \bfi^\prime)$, etc. The application of the ODE's right-hand-side to a current state tensor $\rho_{\bfi,\bfi^\prime}$ gives the output $y^{out}$ as follows:
  \begin{align} 
       y^{out}_{\bfi,\bfi^\prime}  &= 
       \left(-i h_d(\bfi) + ih_d(\bfi^\prime)
        + l_2(\bfi,\bfi^\prime) + l_1^{\text{diag}}(\bfi, \bfi^\prime) \right) \rho_{\bfi,\bfi^\prime} \\ \label{eq:applyRHS1}
        &+ \sum_k \gamma_{1k} \sqrt{(i_k+1)(i_k^\prime+1)} \rho_{E_{+k}\bfi, E_{+k}\bfi^\prime}\\
        &+ \sum_k \left(-ip^k(t) + q^k(t)\right) \sqrt{i_k+1} \rho_{\Epk \bfi, \bfi^\prime} \\
        &+ \sum_k \left( ip^k(t) + q^k(t)\right) \sqrt{i_k^\prime+1} \rho_{\bfi,\Epk \bfi^\prime} \\
        &+ \sum_k \left(-ip^k(t) - q^k(t)\right) \sqrt{i_k} \rho_{\Emk \bfi, \bfi^\prime} \\
        &+ \sum_k \left( ip^k(t) - q^k(t)\right) \sqrt{i_k^\prime} \rho_{\bfi,\Emk \bfi^\prime}  \label{eq:applyRHS2}
  \end{align}
  where 
  \begin{align}
    h_d(\bfi) &= -\sum_k \frac{\xi_k}{2}(i_k^2 - i_k) - \sum_{l<k} \xi_{lk} (i_l i_k) \\
    l_2(\bfi,\bfi^\prime) &= \sum_k \gamma_{2k} \left(i_ki_k^\prime - \frac 12 \left( (i_k)^2 + (i_k^\prime)^2\right) \right)\\
    l_1^{\text{diag}}(\bfi, \bfi^\prime) &= \sum_k -\frac{\gamma_{1k}}{2}\left(i_k + i_k^\prime\right) 
  \end{align}

  These operations will be applied for all $i_0,\dots, i_Q, i_0^\prime, \dots, i_Q^\prime$. It is important to access the data storage is a linear fashion, if possible. Iterating for all $\bfi, \bfi^\prime$ should therefore start with the slowest moving index in the most outer loop. For example, consider the mapping from the stored vectorized $q := vec(\rho)$ to the tensor $\rho_(\bfi,\bfi^\prime)$ to be
    \begin{align}
        \rho(i_1,\dots,i_Q,i_1^\prime,\dots,i_Q^\prime) = q[i_1 \frac{N}{n_1} + i_2\frac{N}{n_1n_2} + i_3\frac{N}{n_1n_2n_3} + \dots + i_1^\prime N\frac{N}{n_1} + i_2^\prime N\frac{N}{n_1n_2} + \dots]
    \end{align}
  the the slowest moving index is $i_1^\prime$, followed by $i_2^\prime, \dots, i_Q^\prime$, then $i_1, \dots i_Q$ with $i_Q$ being the fastest moving index. Hence, the most outer loop should iterate over $i_1^\prime$, then moving backwards through this order until $i_Q$ being the most inner iteration loop.

 \subsection{Parallelization} 
 The terms that contain off-diagonals will require communication to access an index $i_k+1$ from $i_k$. We can consider to realize parallel application by storing and communicating ghost-layers for the corresponding dimensions. 


\section{AD-based derviative}
To compute the adjoint action of the ODE right-hand-side to an input adjoint variable $\bar y_{\bfi, \bfi^p}$, consider derivatives based on discrete Automatic Differentiation applied to the update formula \eqref{eq:applyRHS1} -- \eqref{eq:applyRHS2}:
\begin{align} 
       \bar \rho_{\bfi, \bfi^\p} &\pluseq \left(-i h_d(\bfi) + ih_d(\bfi^\prime)
        + l_2(\bfi,\bfi^\prime) + l_1^{\text{diag}}(\bfi, \bfi^\prime) \right) \bar y_{\bfi,\bfi^\prime} \\
        \bar \rho_{E_{+k}\bfi, E_{+k}\bfi^\prime} &\pluseq \sum_k \gamma_{1k} \sqrt{(i_k+1)(i_k^\prime+1)} \bar y_{\bfi, \bfi^\p} \\
        \bar \rho_{\Epk \bfi, \bfi^\prime} &\pluseq \sum_k \left(-ip^k(t) + q^k(t)\right) \sqrt{i_k+1} \bar y_{\bfi, \bfi^\p} \\
        \bar \rho_{\bfi,\Epk \bfi^\prime} &\pluseq \sum_k \left( ip^k(t) + q^k(t)\right) \sqrt{i_k^\prime+1} \bar y_{\bfi, \bfi^\p} \\
        \bar \rho_{\Emk \bfi, \bfi^\prime} &\pluseq \sum_k \left(-ip^k(t) - q^k(t)\right) \sqrt{i_k} \bar y_{\bfi, \bfi^\p} \\
        \bar \rho_{\bfi,\Emk \bfi^\prime} &\pluseq \sum_k \left( ip^k(t) - q^k(t)\right) \sqrt{i_k^\prime} \bar y_{\bfi, \bfi^\p}
  \end{align}
which can be written in terms of updates for $\bar \rho_{\bfi, \bfi^\p}$ as follows
\begin{align} 
       \bar \rho_{\bfi, \bfi^\p} &\pluseq \left(-i h_d(\bfi) + ih_d(\bfi^\prime)
        + l_2(\bfi,\bfi^\prime) + l_1^{\text{diag}}(\bfi, \bfi^\prime) \right) \bar y_{\bfi,\bfi^\prime} \\
         &+ \sum_k \gamma_{1k} \sqrt{(i_k)(i_k^\prime)} \bar y_{E_{-k}\bfi, E_{-k}\bfi^\p} \\
         &+ \sum_k \left(-ip^k(t) + q^k(t)\right) \sqrt{i_k} \bar y_{\Emk\bfi, \bfi^\p} \\
         &+ \sum_k \left( ip^k(t) + q^k(t)\right) \sqrt{i_k^\prime} \bar y_{\bfi, \Emk \bfi^\p} \\
         &+ \sum_k \left(-ip^k(t) - q^k(t)\right) \sqrt{i_k+1} \bar y_{\Epk \bfi, \bfi^\p} \\
         &+ \sum_k \left( ip^k(t) - q^k(t)\right) \sqrt{i_k^\prime+1} \bar y_{\bfi, \Epk\bfi^\p}
  \end{align}
for all $\bfi, \bfi^\p$.
The control functions $p^k, q^k$ depend on optimization parameters $\alpha^k$: $p^k = p(\alpha^k,t), q = q(\alpha^k,t)$. To compute the derivative with respect to those parameters, the same AD-base method can be applied to update $\bar \alpha^k$ from each of the terms.


\section{Performance}
Table \ref{tab:runtime_mac} and \ref{tab:runtime_quartz} compare runtimes of the matrix-free tensor contraction solver to the sparse-matrix implementation that explicitly computes and stores the sparse matrices. The tests run the Alice-Cavity cooling problem, integrating for $N=10000$ time-steps in $[0,1]us$, using the implicit midpoint rule and Petsc's GMRES linear solver, Controls are initialized constant, driving the $|2>$ to the $|1>$ state (approximately). GMRES takes $5$ iterations on average. 
Runtimes and speeups are reported for one sequential evaluation of the objective function (evalF) as well as the gradient (evalGradF), for different problem sizes. Two different platforms: MacBook Pro (2017, 2.3 GHz Intel Core i5), table \ref{tab:runtime_mac} and on LC's Quartz (2.1 GHz Intel Xeon E5-2695), table \ref{tab:runtime_quartz}.

% MACBOOK
\begin{table}[h]
  \begin{tabular}{l|lr|lr}
    \hline\noalign{\smallskip}
         & evalF &    &  evalGradF &\\
         & sparse-mat & mat-free & sparse-mat& mat-free  \\
    \noalign{\smallskip}\hline\noalign{\smallskip}
    2x2  & 3.42   & 0.27 \textcolor{red}{(12.6x)}  & 8.35  &  0.28 \textcolor{red}{(29.8x)}  \\
    3x10 & 16.64  & 1.30  \textcolor{red}{(12.8x)} & 48.29 &  4.25 \textcolor{red}{(11.3x)}  \\
    3x20 &  50.87 & 5.54  \textcolor{red}{(9.2x)}  & 151.81 & 16.83 \textcolor{red}{ (9.0x)} \\
    \noalign{\smallskip}\hline
  \end{tabular}
  \caption{MacBook: Runtimes (sec) for one objective function and one gradient evaluation. Numbers in red parenthesis are speedup of matrix-free over sparse-matrix implementation.}
  \label{tab:runtime_mac} 
\end{table}

% QUARTZ, pbatch
\begin{table}[h]
  \begin{tabular}{l|lr|lr}
    \hline\noalign{\smallskip}
         & evalF &    &  evalGradF & \\
         & sparse-mat & mat-free & sparse-mat& mat-free  \\
    \noalign{\smallskip}\hline\noalign{\smallskip}
    2x2  &  2.14 & 0.52  \textcolor{red}{ (4.1x)} &  5.84 & 0.86 \textcolor{red}{ (6.7x)}  \\
    3x10 &  6.84 & 2.80  \textcolor{red}{ (2.4x)} & 21.75 & 7.61 \textcolor{red}{(2.8x)}   \\
    3x20 & 18.76 & 9.76  \textcolor{red}{(1.9x)}  & 59.97 & 27.95 \textcolor{red}{(2.1x)}  \\
    \noalign{\smallskip}\hline
  \end{tabular}
  \caption{Quartz: Runtimes (sec) for one objective function and one gradient evaluation. Numbers in red parenthesis are speedup of matrix-free over sparse-matrix implementation.}
  \label{tab:runtime_quartz} 
\end{table}

\end{document}