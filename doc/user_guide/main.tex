\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath}
\usepackage{dsfont}
\usepackage{graphicx}
\usepackage[utf8]{inputenc}
\usepackage{xcolor}
\usepackage{soul}
\usepackage{authblk}
% \usepackage[legalpaper, margin=2in]{geometry}
\parindent0pt
\parskip 1.5ex plus 1ex minus .5ex

\definecolor{Blue}{rgb}{0,0,1}                                                     
\definecolor{Red}{rgb}{1,0,0}                                                      
\definecolor{Green}{rgb}{0,1,0}                                                    
\definecolor{Bronze}{rgb}{0.8,0.5,0.2}                                             
\definecolor{Violet}{rgb}{0.54,0.17,0.89}                                          
                                                                                   
\newcommand{\TODO}[1]{{\textcolor{Violet}{TODO: #1}}}                              
\newcommand{\YC}[1]{{\textcolor{Bronze}{#1}}}                                     
\newcommand{\SG}[1]{{\textcolor{Blue}{#1}}}

\DeclareMathOperator{\Tr}{Tr}
\newcommand{\Ell}{\mathcal{L}}
\newcommand{\R}{\mathds{R}}
\newcommand{\C}{\mathds{C}}
\newcommand{\bfa}{\boldsymbol{\alpha}}


\newcommand\blfootnote[1]{%
  \begingroup
  \renewcommand\thefootnote{}\footnote{#1}%
  \addtocounter{footnote}{-1}%
  \endgroup
}

\title{Quandary: Optimal Control for Quantum Systems}
\author{Stefanie G{\"u}nther\thanks{Center for Applied Scientific Computing, Lawrence Livermore
    National Laboratory, Livermore, CA, USA.} \and N. Anders Petersson$^*$} 
% \date{April 7, 2021}
\date{last updated April 12, 2021}

\begin{document}
\maketitle


\section{Introduction}
Quandary numerically simulates and optimizes the time evolution of open quantum systems. The
underlying dynamics are modeled by Lindblad's master equation, a linear ordinary differential
equation (ODE) describing quantum systems interacting with the environment. Quandary solves this ODE
numerically by applying a time-stepping integration scheme and applies a gradient-based optimization
scheme to determine optimal control pulses that drive the quantum system to a desired target state.
Two optimization objectives are considered: (a) Unitary gate optimization that finds controls to
realize a logical quantum gate, and (b) optimal reset that aims to drive the quantum system to the
ground states.
% Gradient-based optimization schemes utilizing Petsc's Tao optimization package are applied to
% generate control pulses that minimize the respective measure.
To mitigate excessive execution run times, Quandary implements various parallelization strategies based on MPI, including distributed initial conditions, distributed linear algebra using Petsc, as well as (experimental) parallelization in the time domain utilizing the XBraid software.
%can be build to link with the XBraid software
% library which provides a parallelization strategy to distribute the time-evolution of the underlying
% dynamics onto multiple processor applying a parallel-in-time multigrid reduction scheme.

This document outlines the mathematical background and underlying equations, and summarizes their
implementation in Quandary. A full documentation is under development. In the meantime, don't
hesitate to direct any questions to guenther5@llnl.gov. For installation instructions, please take a
look at the README.md document in the \verb+quandary+ directory.

\section{Model equation}
Quandary models open quantum systems with $Q$ subsystems with $n_k$ levels for the
$k$-th subsystem, $k=0,\dots,Q-1$. The time-evolution of the open quantum system is modeled by Lindblad's master equation:
\begin{align}\label{mastereq}
  \dot \rho(t) = &-i(H(t)\rho(t) - \rho(t)H(t)) + \Ell(\rho(t))
\end{align}
for the density matrix $\rho(t)\in \C^{N\times N}$ with dimension $N :=
\prod_{k=0}^{Q-1} n_k$, where $H(t)$ denotes the Hamiltonian describing the system and control and $\Ell(\rho(t))$ denotes the Lindbladian operator that models system-environment interactions as specified below.

The Hamiltonian $H(t)$ is decomposed into a time-independent system part $H_d$ and a time-varying control part ($H_c(t)$) that employs the external control fields: $H(t) = H_d + H_c(t)$. The system Hamiltonian is modeld as 
\begin{align}
  H_d &:= \sum_{k=0}^{Q-1} \left(\omega_k a_k^{\dagger}a_k- \frac{\xi_k}{2} a_k^{\dagger}a_k^{\dagger}a_k a_k  + \sum_{l> k} \left(  J_{kl} \left( a_k^\dagger a_l + a_k a_l^\dagger \right) -\xi_{kl} a_{k}^{\dagger}a_{k}   a_{l}^{\dagger} a_{l} \right)\right)
\end{align}
Here, $\omega_k>0$ denotes the $0 \rightarrow 1$ transition frequency and $\xi_k>0$ is the self-Kerr coefficient of subsystem $k$. The cross resonance coefficients are $J_{kl}>0$ (``dipole-dipole interaction'') and $\xi_{kl}>0$ (``zz-coupling''). Further,
$a_k\in \C^{N\times N}$ denotes the lowering operator acting on subsystem $k$, which is defined as
\begin{align}
  \begin{array}{rl}
  a_0 &:= a^{(n_0)} \otimes I_{n_1} \otimes \dots \otimes
  I_{n_{Q-1}}\\
  a_1 &:= I_{n_0} \otimes a^{(n_1)} \otimes \dots \otimes
  I_{n_{Q-1}}\\
  \vdots \, & \\
  a_{Q-1} &:= I_{n_0} \otimes I_{n_1} \otimes \dots \otimes
  a^{(n_{Q-1})}\\
  \end{array}
  \quad \text{with}\quad
 a^{(n_k)} = \begin{pmatrix}
   0 & 1 &          &         &    \\
     & 0 & \sqrt{2} &         &     \\
     &   & \ddots   & \ddots  &    \\
     &   &          &         & \sqrt{n_k-1}  \\
     &   &          &         & 0   
 \end{pmatrix} \in \R^{n_k \times n_k}
\end{align}
where $I_{n_k} \in \R^{n_k \times n_k}$ is the identity matrix.

The action of external control fields on the quantum system is modelled through the control Hamiltonian 
\begin{align}
  H_c(t) &:= \sum_{k=0}^Q f^k(\vec{\alpha}^k,t) \left(a_k + a_k^\dagger \right)
\end{align}
where $f^k(\vec{\alpha}^k,t)$ are real-valued, time-dependent control functions that are parameterized by real-valued parameters $\vec{\alpha}^k\in \R^d$, which are to be determined through optimization. 

The Lindbladian operator $\Ell(\rho(t))$ is assumed to be of the form 
\begin{align}
  \Ell(\rho(t)) = \sum_{k=0}^{Q-1} \sum_{l=1}^2 \Ell_{lk} \rho(t)
  \Ell_{lk}^{\dagger} - \frac 1 2 \left( \Ell_{lk}^{\dagger}\Ell_{lk}
  \rho(t) + \rho(t)\Ell_{lk}^{\dagger} \Ell_{lk}\right).
\end{align}
Here, the collapse operators $\Ell_{lk}$ model decay and dephasing processes in the subsystem with $\Ell_{1k} = \frac{1}{\sqrt{T_1^k}} a_k$ (``T1'' -- decay) and $\Ell_{2k} = \frac{1}{\sqrt{T_2^k}} a_k^{\dagger}a_k$ (``T2'' -- dephasing) for each subsystem $k$. The constants $T_l^k>0$ correspond to the half-life of process $l$ on subsystem $k$. Typical T1 decay time is between $10-100$ microseconds (us). T2 dephasing time is typically about half of T1 decay time. Decay processes typically behaves like $\exp(-t/{T_1})$.



\subsection{Rotational frame approximation}
In order to slow down the time-scales in the solution of Lindblad's master equations, the rotating wave approximation is applied. To that end, the real-valued laboratory frame control functions is written as 
\begin{align}
  f^k(\vec{\alpha}^k,t) = 2\mbox{Re}\left(d^k(\vec{\alpha}^k,t)e^{i\omega_k^r t}\right), \quad d^k(\vec{\alpha}^k,t) = p^k(\vec{\alpha}^k,t) + i q^k(\vec{\alpha}^k,t)
\end{align}
for real-valued rotating-frame control functions $p^k(\vec{\alpha}^k,t)$ and $q^k(\vec{\alpha}^k,t)$, using rotating-frame frequencies $\omega_k^r$. Under the rotating frame wave approximation, the Hamiltonians are transformed to 
\begin{align} 
  \tilde H_d(t) &:= \sum_{k=0}^{Q-1} \left(\omega_k - \omega_k^{r}\right)a_k^{\dagger}a_k- \frac{\xi_k}{2}
  a_k^{\dagger}a_k^{\dagger}a_k a_k  
   - \sum_{l> k} \xi_{kl} a_{k}^{\dagger}a_{k}   a_{l}^{\dagger} a_{l} \notag \\
   & + \sum_{k=0}^{Q-1}\sum_{l>k} J_{kl} \left(\cos(\eta_{kl}t) \left(a_k^\dagger a_l + a_k a_l^\dagger\right) + i\sin(\eta_{kl}t)\left(a_k^\dagger a_l - a_k a_l^\dagger\right) \right) \label{eq:Hd_rotating} \\
   %
   \tilde H_c(t) &:= \sum_{k=0}^{Q-1} \left( p^k(\vec{\alpha}^k,t) (a_k +
   a_k^{\dagger}) + i q^k(\vec{\alpha}^k,t)(a_k - a_k^{\dagger})
   \right)  \label{eq:Hc_rotating}
\end{align} 
where $\eta_{kl} := \omega_k^{r} - \omega_l^{r}$ are the differences in rotational frequencies between subsystems. When $\omega_k >> \xi_k > \xi_{kl}$ and $|d^k|_{\infty} << 1$, the spectrum of the Hamiltonian in the laboratory frame is dominated by the terms $\omega_ka_k^\dagger a_k$. In this case, the eigenvalues of the rotating frame Hamiltonian become significantly smaller in magnitude by choosing $\omega_k^r \approx \omega_k$, thus slowing down the time variation of the state evolution. We remark that the rotating wave approximation ignores terms in the control Hamiltonian that oscillate with frequencies $\pm 2\omega_k^r$. 

In the remainder of this document, we drop the tildes on $\tilde H_d$ and $\tilde H_c$ and use the rotating frame definition of the Hamiltonians to model the system evolution in time. 

Note, that in the general case the system Hamiltonian in the rotational frame is now time-dependent. However, if one chooses the rotational frequencies to be the same for each subsystem, for example choosing $\omega_k^{r} = \omega_{avg} \, \forall \, k$ with $\omega_{avg} := \frac 1Q \sum_k \omega_k$, the rotational-frame system Hamiltonian $H_d$ becomes time-independent again since $\eta_{kl} = 0$. 

\subsection{Control pulses} \label{subsec:controlpulses}
The time-dependent rotating-frame control functions $d^k(\vec{\alpha}^k,t)$ are parameterized using $N_s$ piecewise quadratic B-spline basis functions $B_s(t)$ acting as envelope for $N_f^k$ carrier waves:
\begin{align}
  d^k(\vec{\alpha}^k,t) = \sum_{f=1}^{N_f^k} \sum_{s=1}^{N_s} \alpha_{s,f}^k B_s(t) e^{i\Omega_k^ft}, \quad \alpha_{s,f}^k = \alpha_{s,f}^{k(1)} + i \alpha_{s,f}^{k(2)} \in \C
\end{align}
The amplitudes $\alpha_{s,f}^{k(1)}, \alpha_{s,f}^{k(2)} \in \R$ are the control
parameters (\textit{design} variables) that Quandary can optimize in order to realize a
desired system behavior, giving a total number of $2N_s\sum_kN_f^k$ real-valued optimization variables. Further, $\Omega_k^f \in \R$ denote the carrier wave frequencies in the rotating frame which can be choosen to trigger certain system frequencies. The corresponding Lab-frame carrier frequencies become $\omega_k^r + \Omega_k^f$. Those frequencies can be chosen to match the transition frequencies in the lab-frame system Hamiltonian. For example, when $\xi_{kl} << \xi_k$, the transition frequencies satisfy $\omega_k - n\xi_k$. Thus by choosing $\Omega_k^f = \omega_k-\omega_k^r - n \xi_k$, one triggers transition between energy levels $n$ and $n+1$ in subsystem $k$. 

Using trigonometric identities, the real and imaginary part of the rotating-frame control $d^k(\vec{\alpha}^k,t) = p^k(\vec{\alpha}^k,t) + iq^k(\vec{\alpha}^k,t)$ are given by
\begin{align}
  p^k(\vec{\alpha}^k,t) &= \sum_{f=1}^{N_f^k} \sum_{s=1}^{N_s} B_s(t)
  \left(\alpha^{k
  (1)}_{s,f} \cos(\Omega_f^k t) - \alpha^{k (2)}_{lsf} \sin(\Omega_f^k t)
  \right) \\
  q^k(\vec{\alpha}^k,t) &= \sum_{f=1}^{N_f^k} \sum_{s=1}^{N_s} B_s(t)\left( \alpha^{k
  (1)}_{s,f} \sin(\Omega_f^k t) + \alpha^{k (2)}_{s,f} \cos(\Omega_f^k t)
  \right)
\end{align}
Those relate to the Lab-frame control $f^k(\vec{\alpha}^k,t)$ through
% \begin{align}
%   f^k(t) = 2 \sum_{l=0}^{L-1} B_l(t) \sum_{f=0}^{N_f^k-1} \beta_{l,f}^k \cos(\omega_k^{\text{rot}} t +
%   \Omega_f^k t + \theta_{l,f}) \ \forall\  k=0,\dots Q-1
% \end{align}
% where $\beta_{l,f}^k$ and $\theta_{l,f}$ are computed from
% \begin{align}
%   \alpha_{l,f}^{k(0)} = \beta_{l,f}^k \cos(\theta_{l,f}) \quad \text{and} \quad
%   \alpha_{l,f}^{k(1)} = \beta_{l,f}^k \sin(\theta_{l,f})
% \end{align}
% and $\omega_k^{\text{rot}}$ is the angular frequency of rotation in subsystem $k$. Using
% trigonometric equalities, the Lab-frame controls can be expressed as
\begin{align}
  f^k(t) &=  2\sum_{f=1}^{N_f^k} \sum_{s=1}^{N_s} B_s(t) \left(\alpha_{s,f}^{k(1)} \cos((\omega_k^{r} + \Omega_f^k) t) - \alpha_{s,f}^{k(2)}\sin((\omega_k^{r} + \Omega_f^k) t) \right) \\
         &= 2 p^k(\vec{\alpha}^k, t) \cos(\omega_k^{r} t) - 2 q^k(\vec{\alpha}^k,
         t)\sin(\omega_k^{r} t) \\
         &= 2\mbox{Re}\left( d^k(\vec{\alpha}^k,t)e^{i\omega_k^r t} \right)
\end{align}


\section{Optimization problem}
In the most general form, Quandary can solve the following optimization problem
\begin{align}
  \min_{\boldsymbol{\alpha}} \frac{1}{n_{init}} \sum_{i=1}^{n_{init}} \beta_i J(\rho_i(T))  + \gamma_1 \| \bfa \|^2_2 + \gamma_2 \int_0^T P\left(\rho_i(t)\right) \, \mathrm{d} t
\end{align}
where $\rho_i(T)$ solves Lindblad's master equation \eqref{mastereq} in the rotating frame subject to the initial condition $\rho_i(0)$, as explained below, and $\boldsymbol{\alpha} = \left(\vec{\alpha}^0, \dots, \vec{\alpha}^{Q-1}\right)$ denotes the control parameters. Specific choices of $J$ and initial conditions $\rho_i(0)$ are given below. 

The second term in the objective is a Tikhonov regularization term that can be added with parameter $\gamma_1\geq 0$ in order to regularize the optimizatin problem (stabilize optimization convergence) by favoring solutions with small norm. 

The third term in the objective function serves as a penalty term that can be added with $\gamma_2 \geq 0$ in oder to achieve a desired behavior of the quantum system over the entire time-domain $0\leq t\leq T$, see below. 


\subsection{Ground-state optimization aka optimal
reset}\label{sec:groundstate-obj}
Optimal reset aims to find control pulses $p^k(\vec{\alpha}^k, t), q^k(\vec{\alpha}^k, t)$,
that drive any initial state towards the ground state $|0\dots 0\rangle = (1, 0,
0, \dots )^T$. The corresponding ground state density matrix is given by
\begin{align}
  \rho_{G} := |0\dots 0\rangle \langle 0 \dots 0 | = 
  \begin{bmatrix} 1      & 0      &  \dots   \\ 
                  0      & 0      &  \dots  \\ 
                  \vdots & \vdots &  \dots 
  \end{bmatrix}
\end{align}

To reach this goal, Quandary either minimizes the distance between the final state
density matrix and the ground-state density matrix, measured in the Frobenius
norm, or minimizes a weighted average of the expected energy level over each subsystem:
\begin{enumerate}
  \item \textit{Expected energy minimization}: Minimize a weighted sum over
    expected energy levels of the (full or partial) system
    \begin{align}
       \quad J(\rho(T)) = \sum_{k\in \mathcal{K}} \langle O^k \rangle 
    \end{align}
    using the observable $O^k = a_k^\dag a_k$ for the subsystem $k$, and defining a partial system
    $H_{k_0}\otimes \dots \otimes H_{k_s}$ from an index set of
    subsystem IDs $\mathcal{K}=\{k_0,\dots,k_s\}\subset
    \{0,\dots,Q-1\}$. 
    $O^k$ measures the probability of the energy levels of subsystem $k$. The
   expected energy level of subsystem $k$ is hence given by 
  \begin{align}
    \langle O^k \rangle &= \mbox{Tr}(O^k\rho).
    \label{eq:expected_energy1}
  \end{align}
    
    Quandary can use the following variations of the
    above measure
    \begin{enumerate}
      \item[(a)] $J(\rho(T)) =
        \left(\frac{1}{|\mathcal{K}|}\sum_{k\in\mathcal{K}}\langle O^k \rangle
        \right)^2$
      \item[(b)] $J(\rho(T)) =
        \frac{1}{|\mathcal{K}|}\sum_{k\in\mathcal{K}}\left(\langle O^k \rangle
        \right)^2 $
      \item[(c)] $J(\rho(T)) =
        \frac{1}{|\mathcal{K}|}\sum_{k\in\mathcal{K}}\langle O^k \rangle$
    \end{enumerate}
  \item \textit{Direct ground-state optimization}: Minimize the Frobenius norm
    between the final density matrix and the ground-state density matrix:
    \begin{align}\label{eq:ground-state-obj}
      J(\rho(T)) = \frac 12 \| \rho(T) - \rho_G \|^2_F 
    \end{align}
    where the Frobenius norm is defined as $\|A\|^2_F = \Tr(A^{\dagger}A)$.
\end{enumerate}

\subsubsection{Penalty term for ground-state optimization}
The penalty term for ground-state optimization can be added with parameter $\gamma_2 \geq 0$ in order to discourages non-zero energy states over time:
\begin{align}
  % \int_0^T w(t) J(\rho(t)) \, \mathrm{d}t, \quad \text{where} \quad w(t) =
  % \left(\frac{t}{T}\right)^p, p \geq 1
  \gamma_2 \int_0^T P(\rho(t)) \mathrm{d}t = \gamma_2 \int_0^T w(t) J\left(\rho(t)\right) \, \mathrm{d}t, \quad \text{where} \quad w(t) =
  \frac{1}{a} e^{ -\left(\frac{t-T}{a} \right)^2},
\end{align}
for a parameter $0 < a \leq 1$. Note, that as $a\to 0$, the weight $w(t)$ converges to the Dirac delta distribution with peak at final time $T$, hence reducing $a$ leads to more emphasis on the final time $T$ while larger $a$ penalize non-zero energy states at earlier times $t\leq T$. 

\subsection{Target gate optimization}

Here, the goal is to find control pulses that realize a certain logical gate
operation at final time $T$. Representing the gate by a unitary matrix $V\in
\C^{N\times N}$, the goal is to match the final state $\rho(T)$ to the unitary
transformation $V\rho(0)V^{\dagger}$ for any initial state $\rho(0)$. Two different measures are available:

\begin{align}
 \text{Frobenius norm:} \quad & J(\rho(T)) = \frac{1}{2}\| \rho(T) - V\rho(0)V^{\dagger} \|^2_F  \\
 \text{Trace overlap:} \quad  & J(\rho(T)) = 1 - \frac{1}{\mbox{Tr}\left(\rho(0)^2\right)}\mbox{Tr}\left(V\rho(0)V^{\dagger}\rho(T) \right) 
\end{align} 


Target gates that are currently implemented are
\begin{align}
  V_{X} := \begin{bmatrix} 0 & 1 \\ 1 & 0  \end{bmatrix} \quad
  V_{Y} := \begin{bmatrix} 0 & -i \\ i & 0 \end{bmatrix} \quad
  V_{Z} := \begin{bmatrix} 1 & 0 \\ 0 & -1 \end{bmatrix} \quad 
  V_{Hadamard} := \frac{1}{\sqrt{2}} 
           \begin{bmatrix} 1 & 1 \\ 1 & -1 \end{bmatrix} \\
  V_{CNOT} := \begin{bmatrix} 1  & 0 & 0 & 0 \\ 
                               0  & 1 & 0 & 0 \\ 
                               0  & 0 & 0 & 1 \\ 
                               0  & 0 & 1 & 0 \\ 
                \end{bmatrix} \quad 
  V_{SWAP} := \begin{bmatrix}
    1 & 0 & 0 & 0 \\
    0 & 0 & 1 & 0 \\
    0 & 1 & 0 & 0 \\
    0 & 0 & 0 & 1 \\
  \end{bmatrix}
\end{align}

As a measure of optimization success, Quandary reports on the average gate fidelity computed from 
\begin{align}
  F_{avg} = \frac{1}{N^2} \sum_{i=1}^{N^2} \mbox{Tr}\left(V\rho_i(0)V^{\dagger}\rho_i(T) \right) 
\end{align}
where $\rho_i(0) = B^{k(i), j(i)}$ are the initial basis matrices as defined in \eqref{eq:basismats} and $\rho_i(T)$ their corresponding propagated final time states.


\subsubsection{Essential and guard levels for gate optimization}
It is often useful, to model the quantum system with more energy levels than the number of levels that the target gate is defined on. For example when optimizing for a swap gate transformation with $V_{SWAP}\in\C^{4\times 4}$, one might want to model the quantum system state for each qubit with more than two energy levels in order to (a) model the (infinite dimensional) system with more accuracy by including more levels and (b) allow the system to transition through higher energy levels in order to achieve the target at final time $T$. 
To this end, Quandary provides the option to specify the number of \textit{essential} energy levels $n_k^e \, \forall k=0,\dots,Q-1$, in addition to the number of energy levels $n_k$ as above, with $n_k^e \leq n_k$. In that case, the quantum dynamics are modelled with (more) energy levels with $N=\prod_k n_k$ and $\rho(t)\in \C^{N\times N}$ while the gate is defined in the essential level dimensions $V\in C^{N_e \times N_e}, N_e=\prod_k n_k^e$. 

To compute the objective function at final time T, the essential-dimensional gate is projected upwards to the full dimensions $\tilde V \in N\times N$ by inserting identiy blocks for rows/columns that correspond to a non-essential level of either of the subsystems. Hence, a realization of the gate $\tilde V$ will not alter the occupation of higher (non-essential) energy level compared to their initial occupation at $t=0$. 

When only the three states as initial conditions are considered, those three initial states will be spanned in the full dimensional system. On the other hand, when the basis of initial states is considered, or their diagonals only, those initial states will be spanned only in the essential level dimensions and zero rows and columns will be inserted for all non-essential levels. Hence, the gate will be realized for any initial state that is spanned in the essential level dimensions, and occupations of non-essential levels at $T=0$ are avoided. 
 

Non-essential are often considered to be so-called \textit{guard levels}. Typically, the occupation of those higher energy levels (all levels $m$ with $n_k^e < m \leq n_k$), and in particular the highest energy level that is modelled ($m=n_k$ for each $k$), should be avoided to prevent leakage into higher levels that are not modelled by the finite-dimensional approximation to the infinite dimensional Hamiltonian. During gate optimization, one can add an integral penalty term to the objective function that penalizes occupation of the last energy level per oscillator:
\begin{align}
  \gamma_2 \int_0^T P(\rho(t)) \mathrm{d}t = \gamma_2 \int_0^T \sum_{r} a\| \rho(t)_{rr} \|^2_2 \, \mathrm{d}t
\end{align}
for a (currently fixed) parameter $a>0$. Here $\rho(t)_{rr}$ denotes the diagonal element at $(r,r)$ inside $\rho(t)\in \C^{N\times N}$ and $r$ iterates over all indices of $\rho$ that correspond to a guard level (last energy level) of at least one of the subsystems. Future implementations will include not only the last but all guard levels in the penalty term, weighted with different paramters $a_{m}$ for each guard level. 




\subsection{Initial conditions}\label{subsec:initcond}

\subsubsection{Pure state initialization}
One can choose to simulate and optimize for only one specific initial condition $\rho(0)$. In that case, each evaluation of the objective function solves the master equation once for this specific initial condition ($\Rightarrow n_{init} = 1$). 

In the case of pure-state initialization, the initial density matrix is composed
of Kronecker products of pure states for each of the subsystems. E.g. for a bipartite system with $n_1
\otimes n_2$ levels, one can propagate any initial pure state 
\begin{align}
  \rho(0)  = |a\rangle \langle a| \otimes |b\rangle \langle b| \quad \text{for} \quad a \in \{0,\dots, n_1-1\}, b\in \{0,\dots, n_2-1\}
\end{align}
Alternatively, a specific initial condition can also be read from a file. 

The unique integer identifier for
this initial condition will be $i=-1$ (in order to distinguish from the below
basis elements or diagonals as below). 

\subsubsection{Basis for initial condition density matrices}
Typically, the optimization goal is to find control parameters that minimize the
respective measure for \textit{any} possible initial condition $\rho(0)$. We
therefore define a basis for all initial density matrices and aim to minimize
the average of the above measure over those matrix basis elements. The basis
matrices are defined as follows:

\begin{align}\label{eq:basismats}
B^{kj} := \frac 12 \left( E_{kk} + E_{jj}\right) +  \begin{cases} 
          0 & \text{if} \, k=j \\ 
        \frac 12 \left( E_{kj} + E_{jk}\right) & \text{if} \, k<j \\
        \frac i2 \left( E_{jk} - E_{kj}\right) & \text{if} \, k>j
      \end{cases} 
\end{align}
for all $k,j\in\{0,\dots, N-1\}$, where $E_{kj}$ denotes the matrix of all zeros
except for the $(k,j)$-th element which is one (hence $E_{kj} = e_ke_j^T$).
These $N^2$ matrices are Hermitian, and they are linearly independent over the
real vector space of Hermitian matrices. They further satisfy $\Tr(B^{kj}) = 1$
and they are positive semi-definite, hence each basis matrix is a density
matrix. 

In Quandary, these $N^2$ basis elements can be used to perform the optimization ($\Rightarrow n_{init}=N^2$). To uniquely identify the different initial conditions in the code, we assign a
unique index $i \in \{0,\dots, N^2-1\}$ to each basis elements with 
\begin{align*}
  B^i := B^{k(i), j(i)}, \quad \text{with} \quad k(i) := i \,\mbox{mod}\, N,
  \quad \text{and} \quad j(i) := \left\lfloor \frac{i}{N} \right\rfloor
\end{align*}

% We can rewrite any initial condition $\rho(0)$ as a linear combination 
% \begin{align}
%   \rho(0) = \sum_{k=0}^{N-1} \sum_{j=0}^{N-1} a_{kj} B^{k, j}.
% \end{align}
% for coefficients $a_{kj} \in \R$.
% Since the master equation \eqref{mastereq} is linear, we can formally define a
% linear solution operator $S^{\vec{\alpha}}(t,\rho(0))$, mapping an initial state
% to the state at time $t$, and we can rewrite the final density matrix at time
% $T$ in terms of the basis elements:
% \begin{align}
%   \rho(T) = S^{\vec{\alpha}}(T,\rho(0)) = \sum_{k,j=0}^{N-1} a_{kj}
%   S^{\vec{\alpha}}(T,B^{k, j}).
% \end{align}
% % AP: According to D.~Lidar's lectue notes, eqn. (112), the most general
% expression for the evolution of the density matrix follows from the Kraus
% operator sum representation,
% \[
%   \rho(T) = \sum_{\mu,\nu} K_{\mu\nu}(t) \rho(0) K_{\mu\nu}^\dagger(t),\quad
%   \sum_{\mu,\nu}
%   K_{\mu\nu}^\dagger(t)K_{\mu\nu}(t)=I.
% \]

% We therefore define the optimization problem that minimizes the respective
% measure averaged over all the basis elements:
% \begin{align}\label{optimproblem_basis}
%   \min \frac{1}{N^2} & \sum_{k,j=0}^{N-1} \, J(\rho^{k,j}(T))  \\
%   \text{s.t.} \quad  \text{each} \quad \rho^{k,j}(t) \quad & \text{solves
%   \eqref{mastereq} with initial condition} \quad \rho^{k,j}(0) = B^{k,j}
% \end{align}

% If the objective measure $J(\rho)$ is linear in $\rho$ (as for example in the
% groundstate optimization minimizing the expected energy levels), we can make use
% of the linearity of $S^{\vec{\alpha}}$:
% \begin{align}
%    \frac{1}{N^2}  \sum_{k,j=0}^{N-1} \, J\left(\rho^{k,j}(T)\right) =
%    J\left(S^{\vec{\alpha}}\left(T,\frac{1}{N^2}  \sum_{k,j=0}^{N-1}
%    B^{kj}\right)\right)
% \end{align}
% Hence, we define a new optimization problem using the specific initial condition
% that averages over the basis matrices:
% \begin{align}
%   \min \, J(S^{\vec{\alpha}}(T,\rho(0))) \quad \text{s.t.} \quad \rho(0) =
%   \frac{1}{N^2}  \sum_{k,j=0}^{N-1} B^{kj}
% \end{align}
% In this case, each objective function evaluation requires to propagate only one
% initial condition $\rho(0)$. 

\subsubsection{Diagonals as initial condition}
One can choose to propagate only those basis elements that correspond to pure states. In that case, only the basis matrices that correspond
to diagonal elements ($B^{mm}, m=1,\dots,N)$ are propagated through the time
domain, and the objective function evaluates the average measure over those $N$
initial conditions at the final time $T$ ($\Rightarrow n_{init} = N$).

% \subsection{Fidelity} 
% For closed systems, the $\rho(t)$ and the solution operator $U(t)$ are unitary. In that case, one can show that 
% \begin{align}
%   \frac{1}{2N^2}\|\rho(T) - V\rho(0)V\|_F^2 = 1 - \frac{1}{N^2}|\Tr(U(T)^{\dagger}V)|^2,
% \end{align}
% which is called the \textit{infidelity}, and the trace-term is the \textit{fidelity}. However, since we include the Lindblad terms, $\rho(T)$ is not unitary, and the above equation is not valid anymore. For defining a fidelity we consider $F(z) := 1 - J(z)$. Note, however, that if the optimized objective is in the order $O(\delta)$, then the error $\epsilon_i$ of a final state component $i$ from the gate is rather of the order $O(\sqrt{\delta})$, since $J$ represents the average squared error $J = \frac{1}{N^2}\sum_i \epsilon_i^2$ and so $\epsilon_i^2 \sim O(\delta)$ on average.

\subsection{Three-state initialization}
According to Koch et al. it is sufficient to utilize only three initial condition to describe a unitary gate, independent of the dimension $N$. Therefore, if one optimizes for realizing a unitary gate, the following three initial states can be chosen to be considered in Quandary:
\begin{align}
    \rho(0)_0 &= \sum_{i=0}^{N-1} \frac{2(N-i+1)}{N(N+1)} |i\rangle\langle i|\\
    \rho(0)_1 &= \sum_{ij=0}^{N-1} \frac{1}{N} |i\rangle\langle j|\\
    \rho(0)_2 &= \frac{1}{N} I_N
\end{align}


\subsection{Reading an initial condition from file}
A specific initial condition can be read from file ($\Rightarrow n_{init}=1$ with unique identifier $-1$). Format: one column being the vectorized density matrix, first all real parts, then all imaginary parts. 



\section{Implementation}

  \subsection{Vectorization}
  The Lindblad master equation \eqref{mastereq} is in matrix form, describing
  the evolution of the density matrix $\rho = (\rho_1, \dots, \rho_N) \in
  \C^{N\times N}$. In order to solve this numerically, we vectorize the equation
  to receive an ODE for $q(t) := \text{vec}(\rho(t)) \in \C^{N^2}$. Using the
  relations
  \begin{align}
   \text{vec}(AB) &= (I_N\otimes A)\text{vec}(B) = (B^T\otimes I_N)\text{vec}(A)
    \\
   \text{vec}(ABC) &= (C^T\otimes A)\text{vec}(B)
  \end{align}
  for square matrices $A,B,C\in\C^{N\times N}$, we can derive the vectorized
  form of the Lindblad master equation:
  \begin{align}\label{mastereq_vectorized}
    &\dot q(t) = M(t) q(t) \quad  \text{where} \\
    &M(t) := -i(I_N\otimes H(t) - H(t)^T \otimes I_N) + \sum_{k=0}^{Q-1}\sum_{l=1}^2 \gamma_{lk}
    \left( \Ell_{lk}\otimes \Ell_{lk} - \frac 1 2 \left( I_N\otimes
    \Ell^T_{lk}\Ell_{lk} + \Ell^T_{lk}\Ell_{lk} \otimes I_N \right) \right)
  \end{align}
   with $M(t) \in \C^{N^2\times N^2}$, and $H(t) = H_d(t) + H_c(t)$ being the rotating frame system and control Hamiltonians as in \eqref{eq:Hd_rotating} and \eqref{eq:Hc_rotating}, respectively.


  \subsection{Real-valued, vectorized system}
   Quandary solves the vectorized master equation \eqref{mastereq_vectorized} in
   real-valued variables with $q(t) = u(t) + iv(t)$, evolving the real-valued
   states $u(t), v(t)\in \R^{N^2}$ with
   \begin{align}
     \dot q(t) = M(t) q(t) \quad \Leftrightarrow \quad \begin{bmatrix} \dot u(t) \\ \dot v(t) \end{bmatrix} = 
   \begin{bmatrix} A(t) & -B(t) \\ B(t) & A(t) \end{bmatrix} 
   \begin{pmatrix} u(t) \\ v(t) \end{pmatrix} 
   \label{realvaluedODE}
   \end{align}
   for real and imaginary parts $A(t) = \mbox{Re} \left(M(t)\right)$ and $B(t) = \mbox{Im}\left(M(t)\right)$. 

The real and imaginary parts of $q(t)$ are stored in a colocated manner: For
  $q = u+iv$ with $u,v\in\R^{N^2}$, a vector of size $2N^2$ is stored that
  staggers real and imaginary parts behind each other for each component:
  \begin{align*}
    q = u+iv = \begin{bmatrix}
     u^1\\u^2\\ \vdots \\ u^{N^2-1} 
    \end{bmatrix}
    + i \begin{bmatrix}
     v^1\\v^2\\ \vdots \\ v^{N^2-1} 
    \end{bmatrix}
    \quad \Rightarrow \quad
    q_{store} = \begin{bmatrix}
      u_1 \\ v_1\\ u_2 \\ v_2 \\ \vdots \\ u_{N^2-1} \\ v_{N^2-1}
    \end{bmatrix}
  \end{align*}
  In order to access the real and imaginary parts of the $i^{th}$ component of
  $q$, one has to access the elements of $q_{store}$ at index $2i$ and $2i+1$,
  respectively. 


   To assemble (evaluate) 
   $A(t) = Re(M(t))$ and $B(t) = Im(M(t))$, consider
   \begin{align}
    iH &= iH_d(t) + iH_c(t) \\
      &= i\left( \sum_k (\omega_k - \omega_k^{\text{rot}}) a_k^\dagger a_k - \frac{\xi}{2}a_k^\dagger a_k^\dagger a_k a_k  - \sum_{l>k}  \xi_{kl} a_k^\dagger a_k a_l^\dagger a_l +  \sum_{l>k} J_{kl} \cos(\eta_{kl}t)\left(a_k^\dagger a_l + a_ka_l^\dagger \right)
       \right.\\
      & \left. \quad \quad + \sum_k p^k(\vec{\alpha}^k,t) \left(a_k + a_k^\dagger\right) \right)\\
      &+ \left( \sum_k \sum_{kl} - J_{kl} \sin(\eta_{kl}t) \left(a_k^\dagger a_l - a_ka_l^\dagger\right)  - \sum_k q^k(\vec{\alpha}^k, t)\left(a_k - a_k^\dagger\right)\right)
   \end{align}
   Hence $A(t)$ and $B(t)$ are given by 
   \begin{align}
    A(t) &= A_d + \sum_k  q^k(\vec{\alpha}^k,t) A_c^k + \sum_{l>k} J_{kl} \sin(\eta_{kl}t)  A_d^{kl} \\
   \text{with} \quad  A_d &:= \sum_k \sum_{j=1,2} \gamma_{jk} \left( \Ell_{jk}\otimes\Ell_{jk} - \frac 1 2 \left(I_N \otimes \Ell_{jk}^T\Ell_{jk} + \Ell_{jk}^T\Ell_{jk}\otimes I_N\right) \right)\\
    A_c^k &:=  I_N \otimes \left(a_k - a_k^\dagger\right) - \left(a_k - a_k^\dagger\right)^T\otimes I_N \\
    A_d^{kl} &:=  I_N\otimes \left(a_k^\dagger a_l - a_k a_l^\dagger\right) - \left(a_k^\dagger a_l - a_k a_l^\dagger\right)^T\otimes I_N 
   \end{align}
   and
   \begin{align}
     B(t) &=  B_d + \sum_k p^k(\vec{\alpha}^k,t) B_c^k + \sum_{kl} J_{kl} \cos(\eta_{kl}t)B_d^{kl}\\
     \text{with} \quad B_d &:= \sum_k (\omega_k - \omega_k^{\text{rot}}) \left(-I_N \otimes a_k^\dagger a_k + (a_k^\dagger a_k)^T \otimes I_N \right) - \frac{\xi_k}{2}\left(- I_N \otimes a_k^\dagger a_k^\dagger a_k a_k + (a_k^\dagger a_k^\dagger a_k a_k )^T\otimes I_N\right)  \\
       &\quad - \sum_{l>k}  \xi_{kl} \left(-I_N \otimes a_k^\dagger a_k a_l^\dagger a_l + (a_k^\dagger a_k a_l^\dagger a_l)^T \otimes I_N \right)\\
       B_c^k &:=  - I_N \otimes \left(a_k + a_k^\dagger\right) + \left(a_k + a_k^\dagger\right)^T\otimes I_N \\
       B_d^{kl} &:=  - I_N\otimes \left(a_k^\dagger a_l + a_k a_l^\dagger\right) + \left(a_k^\dagger a_l + a_k a_l^\dagger\right)^T\otimes I_N \\
   \end{align}





 \subsection{Sparse-matrix vs. matrix-free solver}

   In Quandary, two versions to evaluate the right hand side of Lindblad's
   equation, $M(t)q(t)$, of the vectorized real-valued system are available:
   \begin{itemize}
     \item \textit{Sparse-matrix solver:}
      The sparse-matrix solver initializes and stores the constant matrices
       $A_d, A_d^{kl}, A_c^k, B_d, B_d^{kl}, B_c^k$ using Petsc's sparse-matrix format. They are used
       as building blocks to evaluate the blocks in the system matrix $M(t)$ with 
     \begin{align}
       A(t) &= Re(M(t)) = A_d + \sum_k q^k(\alpha^k, t)A_c^k + \sum_{l>k} J_{kl} \sin(\eta_{kl}t) A_d^{kl}\\
       B(t) &= Im(M(t)) = B_d + \sum_k p^k(\alpha^k, t)B_c^k + \sum_{kl} J_{kl} \cos(\eta_{kl}t) B_d^{kl}
     \end{align}
   at each time $t$, which are applied to the vectorized, real-valued density matrix using Petsc's sparse MatVec implementation. 

   \item \textit{Matrix-free solver using tensor contractions:}
     As an alternative to the sparse-matrix implementation, a matrix-free tensor-contraction solver is available. In this approach, the matrices $A_d,B_d,
       A_c^k, B_c^k$, etc. and hence $M(t)$ are not stored explicitly, but instead
       their action on a vector $q(t)$ is evaluated. The
       action of those matrices to the system vector is implemented using
       tensor-contractions applied to the corresponding dimension of the density
       matrix. 

     \textbf{Notes}:
     \begin{itemize}
       \item For our current test cases, the matrix-free solver is much faster
         than the sparse-matrix solver (about 10x). However the matrix-free solver
         is currently only implemented for systems consisting of \textbf{two}
         subsystems with $n_1 \otimes n_2$ levels. Future development will
         extend the approach to more than two subsystem. 
       \item The matrix-free solver currently does not parallelize across the
         system dimension (i.e. no parallel Petsc!).
     \end{itemize}


   \end{itemize}

    \subsection{Time-stepping}
    To solve the vectorized master equation \eqref{mastereq_vectorized}, $\dot
    q(t) = M(t) q(t)$ for $t\in [0,T]$, Quandary applies a time-stepping integration
    scheme on a uniform time discretization grid $0=t_0 < \dots t_{N} = T$, with
    $t_n = n \delta t$ and $\delta t = \frac{T}{N}$, and approximates the
    solution at each discrete time step $q^{n} \approx q(t_n)$. The implicit midpoint rule is the default setting, and is the preferred time-stepping integration scheme. 
   
    \subsubsection{Implicit Midpoint Rule (IMR)} 
    The implicit mid-point rule is a symplectic, second-order integration scheme
    of Runge-Kutta type, with a Runge-Kutta tableau given by
    \begin{tabular}{ c | c }
      $1/2$ & $ 1/2$ \\
      \hline
                &  $1$
    \end{tabular}.
    Given a state $q^n$ at time $t_n$, the update formula to compute $q^{n+1}$
    is hence 
    \begin{align}
      q^{n+1} = q^n + \delta t k_1 \quad \text{where} \, k_1 \, \text{solves}
      \quad \left( I-\frac{\delta t}{2} M^{n+1/2} \right) k_1 = M^{n+1/2}  q^n
    \end{align}
    where $M^{n+1/2} := M(t_n + \frac{\delta t}{2})$. In each time-step, we
    first solve a linear equation to get the stage variable $k_1$, then use it
    to update $q^{n+1}$. 

    \subsubsection{Other integrators}
    The code offers the possibility to apply any time-stepping integrators
    provided by the Petsc software package (class TS). Implementation needs to
    be verified. Adjoint time-stepping with Petsc is currently not
    implemented. 

    \subsubsection{Choice of the time-step size}
    In order to choose a time-step size $\delta t$, an eigenvalue analysis of
    the constant drift Hamiltonian $H_d =  -2\pi \left(\sum_{k=0}^{Q-1} \frac{x_k}{2}
    a_k^{\dagger}a_k^{\dagger}a_ka_k + \sum_{l>k} x_{lk}
    a_{k}^{\dagger}a_{k}
    a_{l}^{\dagger}a_{l}\right)$ is considered:
       \begin{align*}  
         \dot u = -i H_d u \qquad \text{with} \quad H_d^{\dagger}  = H_d
       \end{align*} 
       Since $H_d$ is Hermitian, there exists a transformation $Y$ s.t. 
       \begin{align*}
         Y^{\dagger}H_d Y = \Lambda \qquad  \text{where} \quad Y^{\dagger} = Y
       \end{align*}
       where $\Lambda$ is a diagonal matrix containing the eigenvalues of $H_d$.
       Transform $\tilde u = Y^{\dagger} u$, then the ODE transforms to 
       \begin{align*}
         \dot \tilde u = -i \Lambda \tilde u \quad \Rightarrow \dot \tilde u_i =
         -i\lambda_i \tilde u_i \quad \Rightarrow \tilde u_i = a
         \exp(-i\lambda_i t)
       \end{align*}
       Therefore, the period for each mode is $\tau_i =
       \frac{2\pi}{|\lambda_i|}$, hence the shortest period is $\tau_{min} =
       \frac{2\pi}{\max_i\{|\lambda_i|\}}$. If we want $p$ discrete time points
       per period, then $p\delta t = \tau_{min}$, hence 
       \begin{align*}
         \delta t = \frac{\tau_{min}}{p} = \frac{2\pi}{p\max_i\{|\lambda_i|\}}
       \end{align*}
       Usually, for a first order scheme we would use something like $p=20$,
       second order may use $p=10$. 

       If we want to include the time-varying Hamiltonian part $H = H_d +
       H_c(t)$ in the analysis, then we could use the constraints on the control
       parameter amplitudes to remove the time-dependency using their large
       value instead, and then do the same analysis as above. However this
       doesn't ensure that we resolve the time-scale of the fastest control
       function. 
      
\subsection{Optimization}

\subsubsection{Optimization variables and bounds}

Design parameters (optimization variables) are stored in the Quandary code in the following order: List oscillators first $(\vec{\alpha}^0, \dots, \vec{\alpha}^{Q-1})$, then for each $\vec{\alpha}^k \in
\R^{2N_sN_f^k}$, iterate over $N_s$ splines: $\vec{\alpha}^k =
(\alpha^k_1,\dots, \alpha^k_{N_s})$ with $\alpha^k_s \in \R^{2N_f^k}$, then each
$\alpha^k_s$ iterates over carrier waves and for each carrier wave lists
the real and imaginary components: $\alpha^k_s = \alpha^{k(1)}_{s,1}, \alpha^{k(2)}_{s,1},
\dots \alpha^{k(1)}_{s,N_f^k}, \alpha^{k(2)}_{s,N_f^k}$. Hence there are a total of $2N_s\sum_k N_f^k$ real-valued optimization parameters, which are stored in the following order:
  \begin{align}
    \boldsymbol{\alpha} &:= \left( \vec{\alpha}^0, \dots, \vec{\alpha}^{Q-1} \right), \in
    \mathds{R}^{2N_s\sum_k N_f^k} \quad \text{where}\\
    \vec{\alpha}^k &= \left( \alpha_{1,1}^k,\dots, \alpha_{1,N_f^k}^k, \dots,
    \alpha_{N_s,1}^{k}, \dots, \alpha_{N_s,N_f^k}^k \right), \quad \text{with} \quad
    \alpha_{s,f}^k = \left(\alpha_{s,f}^{k(1)}, \alpha_{s,f}^{k(2)} \right) \in
    \R^2,
  \end{align}
  iterating over $Q$ subsystems first, then $N_s$ splines, then $N_f^k$ carrier wave
  frequencies. To access an element $\alpha_{s,f}^{k(i)}$, $i=0,1$, from storage $\bfa$:
  \begin{align}
    \alpha_{s,f}^{k(i)} = \bfa[ \left(\sum_{j=0}^{k-1} 2N_sN_f^j\right) + s*2N_f^k + f*2 + i ],
  \end{align}

  In order to guarantee that the optimizer yields control pulses that are
  bounded with $|p^k(t)| \leq c^k_{max}$, $|q^k(t)| \leq c^k_{max}$ for all
  subsystems $k=0,\dots, Q-1$, we impose box constraints on the control
  parameters:
   \begin{align}
     | \alpha_{s,f}^{k(1)}| \leq \frac{c^k_{max}}{N_f^k} \quad \text{and} \quad |
     \alpha_{s,f}^{k(2)} | \leq \frac{c^k_{max}}{N_f^k}
   \end{align}
   where $N_f^k$ is the number of carrier wave frequencies for subsystem $k$. 

  
  \subsection{Gradient computation via discrete adjoint backpropagation}
   Quandary computes the gradients of the objective function with respect to the design variables $\boldsymbol{\alpha}$ using the discrete adjoint method. The discrete adjoint approach yields exact and consistent gradients on the algorithmic level, at costs that are independent of the number of design variables.    
   To that end, the adjoint approach propagates local sensitivities backwards through the time-domain while concatenating contributions to the gradient using the chain-rule.

  The consistent discrete adjoint time-integration step for
    adjoint variables denoted by $\bar q^{n}$ is given by
   \begin{align}
      \bar q^{n} = \bar q^{n+1} + \delta t \left(M^{n+1/2}\right)^T \bar k_1
      \quad \text{where} \, \bar k_1 \, \text{solves} \quad \left(
      I-\frac{\delta t}{2} M^{n+1/2}\right)^T  \bar k_1 = \bar q^{n+1} 
    \end{align}
  The contribution to the gradient $\nabla J$ for each time step is
    \begin{align}\label{eq:gradient}
      \nabla J += \delta t \left( \frac{\partial M^{n+1/2}}{\partial z}
      \left(q^n + \frac{\delta t}{2} k_1\right) \right)^T\bar k_1
    \end{align}
  
    Each evaluation of the gradient $\nabla J$ involes a forward solve of $n_{init}$ initial quantum states to evaluate the objective function at final time $T$, as well as $n_{init}$ backward solves to compute the adjoint states and the contributions to the gradient. Note that the gradient computation \eqref{eq:gradient} requires the states and adjoint states at each time step, hence the states $q^n$ need to be stored during forward propagation. 



  \subsubsection{Optimization algorithm}
    Quandary utilized Petsc's \textit{Tao} optimization package to apply gradient-based iterative updates to the control variables. The \textit{Tao} optimization interface takes routines to evaluate the objective function as well as the gradient computation. In the current setting in Quandary, Tao applies a nonlinear Quasi-Newton optimization scheme using a preconditioned gradient based on L-BFGS updates to approximate the Hessian of the objective function. A projected line-search is applied to ensure that the objective function yields sufficient decrease per optimization iteration while keeping the control parameters within the prescribed box-constraints. 


    \subsection{Parallelization}
    Quandary offers three levels of parallelization using MPI. 
    \begin{enumerate}
    \item Parallelization over initial conditions: The $n_{init}$ initial conditions $\rho_i(0)$ can be distributed over \textit{np\_init} compute units. Since initial condition are propagated throught the Lindblad solver independently from each other, speedup from distributed initial conditions is ideal. 
    \item Parallel linear algebra with Petsc (sparse-matrix solver only): For the sparse-matrix solver, Quandary utilizes Petsc's parallel sparse matrix and vector storage to distribute vectors onto \textit{np\_petsc} compute units. To perform scaling results, make sure to disable code output (or reduce the output frequency to print only the last time-step), because writing the data files invokes additional MPI calls to gather data on the master node.
    \item Time-parallelization via XBraid (experts only): working implementation, but
      speedup depends on the test case and XBraid setting (experts only),
      configuration option is \textit{np\_braid}
      for the sparse-matrix solver, not for the tensor-contraction solver
    \end{enumerate}
    In the main code, the global communicator (MPI\_COMM\_WORLD) is split into
    three sub-communicator, one for each of the above. The total number of MPI
    processes ($np_{total}$) is split into three subgroups such that 
    \begin{align*}
      np_{braid} * np_{init} * np_{petsc} = np_{total}.
    \end{align*}
    The user specifies the size of the communicator for distributing the
    initial conditions ($np_{init}$) as well as time-parallelization
    ($np_{braid}$) in the config file. The number of cores for parallel linear
    algebra ($np_{petsc}$) is then computed from the above equation. The
    following requirements for parallel distribution must be considered when
    setting up parallel runs:
    \begin{itemize}
    \item $\frac{n_{init}}{np_{init}} \in \mathds{N}$, where $n_{init}$ is
      the number of initial conditions that are considered (being $N^2$ for
      the full basis, $N$ for considering diagonals only as initial
      condition, and $1$ if propagating only one initial condition e.g.
      reading from file, or pure state initialization). This requirement
      ensures that each processor group owns the same number of initial
      conditions.
    \item $\frac{np_{total}}{np_{init}*np_{braid}} \in \mathds{N}$, so that
      each processor group has the same number of cores for Petsc.
    \item $\frac{N^2}{np_{petsc}} \in \mathds{N}$, hence the system
      dimensions must be integer multiple of the number of cores for
      distributing linear algebra in Petsc. This constraint is a little
      annoying, however the current implementation requires this due to the
      colocated storage of the real and imaginary parts of the vectorized
      state.
    \end{itemize}

\section{Regression tests}
Quandary has a set of regression tests. Please take a look at the
REGRESSIONTEST.md document in the \verb+quandary/tests+ directory for 
instruction on how to run the regression tests. 

    \section*{Acknowledgments}
    This work was performed under the auspices of the U.S. Department of Energy by Lawrence
    Livermore National Laboratory under Contract DE-AC52-07NA27344. LLNL-SM-818073. 

This document was prepared as an account of work sponsored by an agency of the United States
government. Neither the United States government nor Lawrence Livermore National Security, LLC,
nor any of their employees makes any warranty, expressed or implied, or assumes any legal
liability or responsibility for the accuracy, completeness, or usefulness of any information,
apparatus, product, or process disclosed, or represents that its use would not infringe
privately owned rights. Reference herein to any specific commercial product, process, or service
by trade name, trademark, manufacturer, or otherwise does not necessarily constitute or imply
its endorsement, recommendation, or favoring by the United States government or Lawrence
Livermore National Security, LLC. The views and opinions of authors expressed herein do not
necessarily state or reflect those of the United States government or Lawrence Livermore
National Security, LLC, and shall not be used for advertising or product endorsement purposes.



\end{document}
